# Megatron-LM

也是model- parallelism，任务切割跟Gpipe不一样。

Megatron，变形金刚里的反派名字。

Gpipe是大的神经网络，这里特指language model，应该就是transformer。

张量并行

方法很简单，性能不错。

我提出一个模型分割的方法，然后训练一个模型，然后效果更好。

更简单，但是丧失了通用性。

提出了一个方法，把模型切开，他这个切法比较好切，也不太会有计算量分配不均的问题，但是，他局限性就是基本上就是只有transformer能用。

通讯量很大，而且不能异步，所以需要很高的带宽。

